import pandas as pd
import numpy as np
from pathlib import Path
from lightgbm import LGBMRegressor
import matplotlib.pyplot as plt
import joblib

# è®¾ç½®è·¯å¾„
BASE_DIR = Path(__file__).resolve().parent
DATA_DIR = BASE_DIR / "qlib_data"

FEATURE_PATH = DATA_DIR / "meta_features_eth_4h.csv"
MODEL_PATH = DATA_DIR / "meta_lightgbm.pkl"


def load_dataset():
    print("ğŸ“¥ Loading dataset...")

    if not FEATURE_PATH.exists():
        raise FileNotFoundError("âŒ æ‰¾ä¸åˆ°ç‰¹å¾æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œ prepare_qlib_data.py")

    # ç›´æ¥è¯»å–ç‰¹å¾æ–‡ä»¶ï¼Œå®ƒå·²ç»åŒ…å«äº†æ‰€æœ‰åˆ—ï¼ˆåŒ…æ‹¬æ”¶ç›Šç‡ï¼‰
    df = pd.read_csv(FEATURE_PATH)

    df["datetime"] = pd.to_datetime(df["datetime"])
    df = df.sort_values("datetime").reset_index(drop=True)

    # ğŸ¯ å®šä¹‰é¢„æµ‹ç›®æ ‡ (Label)
    # æˆ‘ä»¬é¢„æµ‹ "Custom Signal V2" ç­–ç•¥çš„æœªæ¥æ”¶ç›Š
    target_col = "custom_v2_ret" 
    
    if target_col not in df.columns:
        print(f"âš ï¸  æœªæ‰¾åˆ° {target_col}ï¼Œå°è¯•æŸ¥æ‰¾å…¶ä»–æ”¶ç›Šåˆ—...")
        ret_cols = [c for c in df.columns if c.endswith("_ret")]
        if ret_cols:
            target_col = ret_cols[0]
            print(f"ğŸ‘‰ ä½¿ç”¨ {target_col} ä½œä¸ºé¢„æµ‹ç›®æ ‡")
        else:
            raise ValueError("âŒ æ— æ³•æ‰¾åˆ°æ”¶ç›Šç‡åˆ—ä½œä¸ºé¢„æµ‹ç›®æ ‡")

    print(f"ğŸ¯ é¢„æµ‹ç›®æ ‡: {target_col} (Next Bar Return)")
    
    # æ„é€  Label: æœªæ¥ 1 æ ¹ K çº¿çš„æ”¶ç›Š
    df["label"] = df[target_col].shift(-1)
    
    # ç§»é™¤æœ€åä¸€è¡Œï¼ˆå› ä¸ºæ²¡æœ‰ labelï¼‰
    df = df.dropna(subset=["label"])

    # ç‰¹å¾åˆ—ï¼šæ’é™¤ instrument, datetime, label ä»¥åŠæ‰€æœ‰çš„ _ret, _equity, _position åˆ—ï¼ˆé¿å…æœªæ¥å‡½æ•°ï¼‰
    # æ³¨æ„ï¼šæˆ‘ä»¬åªä½¿ç”¨ "è¿‡å»" çš„ä¿¡æ¯ä½œä¸ºç‰¹å¾ã€‚
    # æ’é™¤æ‰€æœ‰åŒ…å« "ret", "equity", "position" çš„åˆ—ï¼Œé™¤éå®ƒä»¬æ˜¯æ»šåŠ¨æŒ‡æ ‡ï¼ˆå¦‚ ret_5, ret_20 ç­‰ï¼Œè¿™äº›æ˜¯è¿‡å»å‘ç”Ÿçš„ï¼Œå¯ä»¥ä½œä¸ºç‰¹å¾ï¼‰
    # ä½†æ˜¯ï¼Œret_5 æ˜¯ "è¿‡å»5æ ¹Kçº¿çš„æ”¶ç›Š"ï¼Œåœ¨Tæ—¶åˆ»æ˜¯å·²çŸ¥çš„ï¼Œæ‰€ä»¥å¯ä»¥ç”¨ã€‚
    # åªæœ‰å½“æœŸçš„ "ret" (å•æ ¹Kçº¿æ”¶ç›Š) æ˜¯æˆ‘ä»¬éœ€è¦é¢„æµ‹çš„ç›®æ ‡ï¼ˆçš„æ»åå€¼ï¼‰ã€‚
    
    # ä¸¥æ ¼æ¥è¯´ï¼ŒTæ—¶åˆ»çš„ ret æ˜¯å·²çŸ¥çš„ã€‚ä½†æ˜¯ä¸ºäº†é¿å…ç›´æ¥æ³„æ¼ï¼ˆæ¯”å¦‚ label = ret.shift(-1)ï¼‰ï¼Œæˆ‘ä»¬è¦å°å¿ƒã€‚
    # è¿™é‡Œçš„ç‰¹å¾å·¥ç¨‹æ˜¯åœ¨ export_strategy_factors.py é‡Œåšçš„ï¼Œret_5 æ˜¯ rolling sumã€‚
    
    exclude_keywords = ["_equity", "_position", "instrument", "datetime", "label"]
    # æ’é™¤å½“æœŸæ”¶ç›Šç‡åˆ— (ä»¥ _ret ç»“å°¾ï¼Œä¸”ä¸æ˜¯ _ret_5, _ret_20 ç­‰)
    # ç®€å•çš„åšæ³•ï¼šæ’é™¤æ‰€æœ‰ä»¥ _ret ç»“å°¾çš„åˆ—ï¼Œåªä¿ç•™ _ret_X
    
    feature_cols = []
    for c in df.columns:
        if c in exclude_keywords or c == target_col:
            continue
        if any(k in c for k in exclude_keywords):
            continue
        
        # å¤„ç† _ret åˆ—
        if c.endswith("_ret"):
            continue # æ’é™¤å½“æœŸæ”¶ç›Š
            
        feature_cols.append(c)
    
    # ç®€å•æ¸…æ´—ï¼šç§»é™¤åŒ…å« infinite çš„è¡Œ
    df = df.replace([np.inf, -np.inf], np.nan).dropna()

    print(f"ğŸ“Š Features: {len(feature_cols)} columns")
    print(f"ğŸ“ˆ Samples: {len(df)} rows")

    return df, feature_cols


def train_model(df, feature_cols):
    X = df[feature_cols]
    y = df["label"]

    # æ—¶é—´åºåˆ—åˆ†å‰² (å‰ 80% è®­ç»ƒï¼Œå 20% æµ‹è¯•)
    split_idx = int(len(df) * 0.8)
    X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]
    y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]
    
    # è®°å½•æµ‹è¯•é›†çš„æ—¶é—´èŒƒå›´
    test_start = df["datetime"].iloc[split_idx]
    test_end = df["datetime"].iloc[-1]
    print(f"ğŸ“… Test Period: {test_start} to {test_end}")

    print("ğŸš€ Training LightGBM model...")

    model = LGBMRegressor(
        n_estimators=1000,
        learning_rate=0.005,
        max_depth=5,
        num_leaves=31,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        n_jobs=-1
    )

    model.fit(
        X_train, y_train,
        eval_set=[(X_test, y_test)],
        eval_metric="rmse",
        callbacks=[
            # lightgbm.early_stopping(stopping_rounds=50)
        ]
    )

    # é¢„æµ‹
    train_pred = model.predict(X_train)
    test_pred = model.predict(X_test)
    
    # è¯„ä¼° IC (Information Coefficient)
    train_ic = np.corrcoef(train_pred, y_train)[0, 1]
    test_ic = np.corrcoef(test_pred, y_test)[0, 1]
    
    # è¯„ä¼° Rank IC
    test_rank_ic = pd.Series(test_pred).corr(pd.Series(y_test.values), method="spearman")

    print(f"\nğŸ“Š Model Performance:")
    print(f"  Train IC: {train_ic:.4f}")
    print(f"  Test IC:  {test_ic:.4f}")
    print(f"  Rank IC:  {test_rank_ic:.4f}")

    return model, X_test, y_test, test_pred


def plot_feature_importance(model, feature_cols):
    print("\nğŸ¨ Plotting feature importance...")

    importance = model.feature_importances_
    # è·å–å‰ 20 ä¸ªé‡è¦ç‰¹å¾
    indices = np.argsort(importance)[-20:]
    
    plt.figure(figsize=(10, 8))
    plt.title("Top 20 Feature Importance (LightGBM)")
    plt.barh(range(len(indices)), importance[indices], align="center")
    plt.yticks(range(len(indices)), [feature_cols[i] for i in indices])
    plt.xlabel("Feature Importance")
    plt.tight_layout()
    
    # ä¿å­˜å›¾ç‰‡
    plt.savefig(DATA_DIR / "feature_importance.png")
    print(f"ğŸ–¼ï¸  Feature importance saved to {DATA_DIR / 'feature_importance.png'}")


def backtest_strategy(y_test, test_pred, df_test):
    """ç®€å•çš„ç­–ç•¥å›æµ‹"""
    print("\nğŸ’° Simple Backtest on Test Set:")
    
    # ç­–ç•¥ï¼šå¦‚æœé¢„æµ‹æ”¶ç›Š > 0ï¼Œåšå¤šï¼›å¦åˆ™ç©ºä»“
    signals = pd.Series(np.where(test_pred > 0, 1.0, 0.0), index=y_test.index)
    
    # è®¡ç®—ç­–ç•¥æ”¶ç›Š
    strategy_ret = signals * y_test
    
    # è®¡ç®—ç´¯è®¡å‡€å€¼
    cum_ret = (1 + strategy_ret).cumprod()
    benchmark_cum_ret = (1 + y_test).cumprod()
    
    final_ret = cum_ret.iloc[-1] - 1
    bench_ret = benchmark_cum_ret.iloc[-1] - 1
    
    print(f"  Strategy Return: {final_ret:.2%}")
    print(f"  Benchmark Return: {bench_ret:.2%}")
    
    # ç®€å•çš„å¤æ™®
    sharpe = strategy_ret.mean() / strategy_ret.std() * np.sqrt(365 * 6)
    print(f"  Strategy Sharpe: {sharpe:.2f}")


def main():
    try:
        df, feature_cols = load_dataset()
        model, X_test, y_test, test_pred = train_model(df, feature_cols)

        # ä¿å­˜æ¨¡å‹
        joblib.dump(model, MODEL_PATH)
        print(f"\nğŸ’¾ Model saved to: {MODEL_PATH}")

        # ç‰¹å¾é‡è¦æ€§
        plot_feature_importance(model, feature_cols)
        
        # ç®€å•å›æµ‹
        split_idx = int(len(df) * 0.8)
        df_test = df.iloc[split_idx:]
        backtest_strategy(y_test, test_pred, df_test)

        print("\nğŸ”¥ Meta-strategy training completed!")
        
    except Exception as e:
        print(f"\nâŒ Error: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
